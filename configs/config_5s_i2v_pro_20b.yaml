metrics:
  scale_factor:
  - 1.0
  - 2.0
  - 2.0
  resolution: 512

model:
  # Path to the 20B DiT weights
  # For diffusers format, point to the actual .safetensors file
  checkpoint_path: "Kandinsky-5.0-I2V-Pro-sft-5s-Diffusers/transformer/diffusion_pytorch_model.safetensors"
  num_steps: 50
  guidance_weight: 5.0

  dit_params:
    # 20B model architecture parameters from transformer config.json
    in_visual_dim: 16
    out_visual_dim: 16
    time_dim: 1024              # 20B: 1024 (vs 2B: 512)
    patch_size:
    - 1
    - 2
    - 2
    model_dim: 4096              # 20B: 4096 (vs 2B: 1792)
    ff_dim: 16384                # 20B: 16384 (vs 2B: 7168)
    num_text_blocks: 4           # 20B: 4 (vs 2B: 2)
    num_visual_blocks: 60        # 20B: 60 (vs 2B: 32)
    axes_dims:
    - 32                         # 20B: 32 (vs 2B: 16)
    - 48                         # 20B: 48 (vs 2B: 24)
    - 48                         # 20B: 48 (vs 2B: 24)
    visual_cond: true
    in_text_dim: 3584
    in_text_dim2: 768
    attention_engine: "sdpa"     # Using full SDPA attention (PyTorch native)
                                 # Other options: flash_attention_2, flash_attention_3, sage

  # Full attention (NABLA sparse attention disabled)
  # To re-enable NABLA sparse attention, uncomment below:
  # attention:
  #   type: nabla
  #   causal: false
  #   local: false
  #   glob: false
  #   window: 3
  #   method: "topcdf"
  #   P: 0.8
  #   add_sta: true
  #   wT: 11
  #   wW: 3
  #   wH: 3

  # VAE configuration
  vae:
    checkpoint_path: "Kandinsky-5.0-I2V-Pro-sft-5s-Diffusers/vae"
    name: "hunyuan"

  # Text embedder configuration for diffusers format
  # Model weights are in text_encoder/, processor files are in tokenizer/
  text_embedder:
    qwen:
      emb_size: 3584
      checkpoint_path: "Kandinsky-5.0-I2V-Pro-sft-5s-Diffusers/text_encoder"
      processor_path: "Kandinsky-5.0-I2V-Pro-sft-5s-Diffusers/tokenizer"
      max_length: 256
    clip:
      checkpoint_path: "Kandinsky-5.0-I2V-Pro-sft-5s-Diffusers/text_encoder_2"
      tokenizer_path: "Kandinsky-5.0-I2V-Pro-sft-5s-Diffusers/tokenizer_2"
      emb_size: 768
      max_length: 77

# Block swapping configuration for 48GB VRAM
# Enable this to fit 20B model in limited memory
block_swap:
  enabled: true
  blocks_in_memory: 6           # Number of visual transformer blocks to keep in GPU (60 total)
                                 # With full SDPA attention on 48GB VRAM, can use 8-12 blocks for faster inference
                                 # Adjust based on available VRAM:
                                 # - 48GB: 8-12 blocks (full attention) or 12-18 blocks (NABLA sparse)
                                 # - 40GB: 6-8 blocks
                                 # - 24GB: 3-4 blocks

# MagCache configuration for 20B Pro I2V model
magcache:
  mag_ratios: [0.95295, 0.96037, 1.11843, 1.10903, 1.01421, 1.01204, 1.01811, 1.01562, 1.01231, 1.01056, 0.9972, 0.99662, 1.01979, 1.01757, 1.02381, 1.02207, 0.99479, 0.9951, 1.00294, 1.0024, 0.99773, 0.9984, 1.01817, 1.01605, 0.99913, 0.99944, 0.99218, 0.99299, 1.03239, 1.03021, 0.99568, 0.9951, 1.0124, 1.01201, 0.97933, 0.98116, 1.02511, 1.02369, 0.98456, 0.98444, 1.01573, 1.01525, 1.01743, 1.01688, 0.99326, 0.99416, 0.99642, 0.99641, 1.03137, 1.0265, 1.00513, 1.00922, 0.97789, 0.98044, 1.02419, 1.02194, 1.00083, 1.00147, 0.99584, 0.9971, 1.01021, 1.00855, 1.03171, 1.02869, 0.97483, 0.97822, 1.03725, 1.02688, 1.00433, 1.00196, 0.99005, 0.99085, 1.01196, 1.01015, 0.99815, 1.00002, 1.00673, 1.00522, 1.00589, 1.00883, 1.00424, 1.00184, 0.9863, 0.98711, 1.00569, 1.00432, 0.99096, 0.98978, 1.00107, 0.99868, 0.97743, 0.97855, 0.96446, 0.96312, 0.93896, 0.9413, 0.85115, 0.84801]
