metrics:
  scale_factor:
  - 1.0
  - 2.0
  - 2.0
  resolution: 512

model:
  # Path to the 10s 20B DiT weights
  checkpoint_path: "./weights/model/kandinsky5pro_t2v_sft_10s.safetensors"
  num_steps: 50
  guidance_weight: 5.0

  dit_params:
    # 20B model architecture parameters
    in_visual_dim: 16
    out_visual_dim: 16
    time_dim: 1024              # 20B: 1024 (vs 2B: 512)
    patch_size:
    - 1
    - 2
    - 2
    model_dim: 4096              # 20B: 4096 (vs 2B: 1792)
    ff_dim: 16384                # 20B: 16384 (vs 2B: 7168)
    num_text_blocks: 4           # 20B: 4 (vs 2B: 2)
    num_visual_blocks: 60        # 20B: 60 (vs 2B: 32)
    axes_dims:
    - 32                         # 20B: 32 (vs 2B: 16)
    - 48                         # 20B: 48 (vs 2B: 24)
    - 48                         # 20B: 48 (vs 2B: 24)
    visual_cond: true
    in_text_dim: 3584
    in_text_dim2: 768
    attention_engine: "sdpa"     # Using SDPA attention (can be overridden)
                                 # Other options: flash_attention_2, flash_attention_3, sage

  # NABLA sparse attention for 10s (critical for memory efficiency with longer sequences!)
  # This enables the model to handle 61 frames (10s) efficiently
  attention:
    type: nabla
    causal: false
    local: false
    glob: false
    window: 3
    P: 0.9                       # Top-k probability threshold
    wT: 11                       # Temporal window (10s needs larger)
    wW: 3                        # Width window
    wH: 3                        # Height window
    add_sta: true                # Add spatial-temporal attention
    method: topcdf               # Top-CDF selection method

  # VAE configuration
  vae:
    checkpoint_path: "./weights/vae/"
    name: "hunyuan"

  # Text embedder configuration
  text_embedder:
    qwen:
      emb_size: 3584
      checkpoint_path: "./weights/text_encoder/"
      max_length: 256
    clip:
      checkpoint_path: "./weights/text_encoder2/"
      emb_size: 768
      max_length: 77

# Block swapping configuration for 48GB VRAM
# REQUIRED for 20B model with 10s (61 frames)
block_swap:
  enabled: true
  blocks_in_memory: 6           # Number of visual transformer blocks to keep in GPU (60 total)
                                 # With NABLA sparse attention on 48GB VRAM:
                                 # - 48GB: 12-18 blocks (sparse attention is more memory efficient)
                                 # - 40GB: 8-12 blocks
                                 # - 24GB: 4-6 blocks
                                 # Start conservative and increase if you have headroom

# MagCache ratios would need to be computed for 10s pro model
# Leave empty for now - can be added after profiling
# magcache:
#   mag_ratios: []
